{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis to identify patterns or instances that do not conform to the expected behavior of a given dataset. These patterns, called anomalies or outliers, can be indicative of errors, fraud, defects, or any other unusual events in the data. The purpose of anomaly detection is to automatically identify such instances that differ significantly from the majority of the data, helping to flag potential issues or interesting events that require further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection faces several challenges, including:\n",
    "\n",
    "Imbalanced Data: Anomalies are typically rare compared to normal instances, leading to imbalanced datasets.\n",
    "Variability: Anomalies can take various forms, making it challenging to define a universal threshold for detecting them.\n",
    "Adaptability: Anomalies can evolve over time, requiring models to adapt to changing patterns.\n",
    "Feature Selection: Identifying relevant features for anomaly detection is crucial.\n",
    "Scalability: Efficiently detecting anomalies in large datasets in real-time can be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised Anomaly Detection: In this approach, the algorithm learns from unlabeled data, aiming to identify patterns that deviate from the norm without using any specific labels for anomalies. It's useful when the nature of anomalies is not well-defined or labeled data is scarce.\n",
    "Supervised Anomaly Detection: This approach uses labeled data, where anomalies are explicitly marked. The algorithm learns the distinction between normal and anomalous instances based on the provided labels. It's effective when there is sufficient labeled data and a clear understanding of what constitutes an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into the following main types:\n",
    "\n",
    "Statistical Methods: Based on statistical properties of the data distribution.\n",
    "Machine Learning Methods: Utilize machine learning techniques to model normal behavior and identify deviations.\n",
    "Clustering-Based Methods: Detect anomalies as instances that do not belong to any cluster.\n",
    "Distance-Based Methods: Detect anomalies based on the distance between data points.\n",
    "Ensemble Methods: Combine multiple models to improve anomaly detection performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods assume that anomalies are typically far away from the majority of normal instances in the feature space. They operate on the principle that normal instances are densely packed, while anomalies are isolated or far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores based on the density of a data point compared to its neighbors. It calculates the local reachability density of each point by comparing the average density of its neighbors with its own density. A point with a significantly lower density than its neighbors is likely an anomaly, as it's in a sparser region of the data space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm, which is an ensemble-based anomaly detection method, has two main parameters:\n",
    "\n",
    "n_estimators: The number of isolation trees in the forest. More trees can lead to better performance but might increase computation time.\n",
    "max_samples: The number of samples drawn to create each isolation tree. A smaller value can improve efficiency but might reduce accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of KNN-based anomaly detection, if a data point has only 2 neighbors within a radius of 0.5, and assuming K=10, the data point's anomaly score would likely be relatively high. Having only 2 neighbors in a relatively large radius suggests that the data point is sparsely surrounded by other points, which is indicative of an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score is inversely proportional to the average path length. An average path length of 5.0 for a specific data point means that, on average, it took 5 splits in the trees to isolate the data point. A lower average path length corresponds to a higher anomaly score. Therefore, a data point with an average path length of 5.0 would have a relatively low anomaly score compared to the average path length of the trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
