{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "P(A ∩ B) = P(A|B) * P(B)\n",
    "P(A ∩ B) = 0.40 * 0.70 = 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are variants of the Naive Bayes algorithm used for text classification and other discrete data classification tasks. The key difference lies in the type of data they are suited for:\n",
    "\n",
    "Bernoulli Naive Bayes: This is used when the features are binary (i.e., presence or absence of a particular feature). It's commonly used for document classification tasks, where each feature represents the presence or absence of a word in the document.\n",
    "\n",
    "Multinomial Naive Bayes: This is used when the features represent counts or frequencies (non-negative integers). It's suitable for tasks involving text data where features could represent word frequencies, such as document classification with term frequency-inverse document frequency (TF-IDF) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes typically assumes binary features (presence or absence of a feature). When dealing with missing values, they are usually treated as a separate category or ignored during model training and classification. In practice, many implementations would convert missing values to \"absent\" (0) during the modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The Gaussian Naive Bayes assumes that the features follow a Gaussian (normal) distribution and can handle continuous numeric features. In the context of multi-class classification, each class would have its own set of Gaussian distribution parameters (mean and variance) for each feature.\n",
    "\n",
    "When dealing with a new instance during classification, the algorithm calculates the probability of the instance belonging to each class based on the Gaussian distributions of the features for that class. The class with the highest probability is chosen as the predicted class for the instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"spambase.data\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
      "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
      "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
      "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
      "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
      "\n",
      "      49   50     51     52     53     54   55    56  57  \n",
      "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
      "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
      "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
      "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
      "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
      "\n",
      "[5 rows x 58 columns]\n",
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "            51, 52, 53, 54, 55, 56, 57],\n",
      "           dtype='int64')\n",
      "                0            1            2            3            4   \\\n",
      "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
      "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
      "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
      "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
      "\n",
      "                5            6            7            8            9   ...  \\\n",
      "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000  ...   \n",
      "mean      0.095901     0.114208     0.105295     0.090067     0.239413  ...   \n",
      "std       0.273824     0.391441     0.401071     0.278616     0.644755  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.160000  ...   \n",
      "max       5.880000     7.270000    11.110000     5.260000    18.180000  ...   \n",
      "\n",
      "                48           49           50           51           52  \\\n",
      "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
      "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
      "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
      "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
      "\n",
      "                53           54           55            56           57  \n",
      "count  4601.000000  4601.000000  4601.000000   4601.000000  4601.000000  \n",
      "mean      0.044238     5.191515    52.172789    283.289285     0.394045  \n",
      "std       0.429342    31.729449   194.891310    606.347851     0.488698  \n",
      "min       0.000000     1.000000     1.000000      1.000000     0.000000  \n",
      "25%       0.000000     1.588000     6.000000     35.000000     0.000000  \n",
      "50%       0.000000     2.276000    15.000000     95.000000     0.000000  \n",
      "75%       0.000000     3.706000    43.000000    266.000000     1.000000  \n",
      "max      19.829000  1102.500000  9989.000000  15841.000000     1.000000  \n",
      "\n",
      "[8 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Explore the column names\n",
    "print(data.columns)\n",
    "\n",
    "# Summary statistics\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting into features (X) and target (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data.iloc[:, -1]   # The last column (target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839\n",
      "Precision: 0.8870\n",
      "Recall: 0.8152\n",
      "F1: 0.8481\n",
      "==============================\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863\n",
      "Precision: 0.7393\n",
      "Recall: 0.7215\n",
      "F1: 0.7283\n",
      "==============================\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8218\n",
      "Precision: 0.7104\n",
      "Recall: 0.9570\n",
      "F1: 0.8131\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform cross-validation and calculate metrics\n",
    "classifiers = [bernoulli_nb, multinomial_nb, gaussian_nb]\n",
    "classifier_names = [\"Bernoulli Naive Bayes\", \"Multinomial Naive Bayes\", \"Gaussian Naive Bayes\"]\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "for clf, clf_name in zip(classifiers, classifier_names):\n",
    "    print(f\"Classifier: {clf_name}\")\n",
    "    for metric in metrics:\n",
    "        scores = cross_val_score(clf, X, y, cv=10, scoring=metric)\n",
    "        average_score = scores.mean()\n",
    "        print(f\"{metric.capitalize()}: {average_score:.4f}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "Based on the results obtained, it appears that the Bernoulli Naive Bayes classifier performed the best among the three variants. It achieved the highest accuracy, precision, recall, and F1 score. The Gaussian Naive Bayes classifier also performed reasonably well but had slightly lower precision compared to the other two metrics.\n",
    "\n",
    "The reason Bernoulli Naive Bayes might have performed the best could be attributed to the nature of the dataset. Since the \"Spambase\" dataset is likely to have binary features (presence or absence of certain words or patterns), the Bernoulli Naive Bayes, which assumes binary features, could have been well-suited for this type of data. It's possible that the features in this dataset align well with the assumptions of the Bernoulli Naive Bayes model.\n",
    "\n",
    "Limitations of Naive Bayes:\n",
    "While Naive Bayes classifiers are simple and efficient, they do have some limitations:\n",
    "\n",
    "Strong Independence Assumption: Naive Bayes assumes that features are conditionally independent given the class. This assumption might not hold in real-world scenarios, leading to reduced accuracy.\n",
    "Sensitive to Feature Correlations: Naive Bayes can struggle when features are correlated, as it treats them as independent.\n",
    "Lack of Tuning Flexibility: Naive Bayes has few hyperparameters to tune, limiting its flexibility in model optimization.\n",
    "Out-of-Distribution Data: If a feature-value combination is not observed in the training data, Naive Bayes assigns it a probability of zero, leading to unreliable predictions for unseen data.\n",
    "Limited Representation: Bernoulli Naive Bayes and Multinomial Naive Bayes are best suited for discrete data, making them less suitable for datasets with continuous features.\n",
    "Conclusion:\n",
    "In this analysis, the Bernoulli Naive Bayes classifier demonstrated better performance on the \"Spambase\" dataset, likely due to its compatibility with the binary nature of the features in the dataset. However, it's important to note that the choice of classifier can heavily depend on the dataset characteristics and assumptions.\n",
    "\n",
    "Future work could involve experimenting with feature engineering techniques, trying different hyperparameter settings, and exploring more sophisticated classifiers to see if further performance improvements can be achieved. Additionally, investigating techniques to address the limitations of Naive Bayes, such as feature correlation handling, could lead to enhanced model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
