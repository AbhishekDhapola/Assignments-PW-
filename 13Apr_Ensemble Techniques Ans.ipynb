{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest Regressor is a machine learning algorithm that belongs to the ensemble learning family. It's used for regression tasks, where the goal is to predict a continuous numeric output based on input features. It's an extension of the Random Forest algorithm, which combines the predictions of multiple decision trees to produce a more accurate and robust prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. How does Random Forest Regressor reduce the risk of overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor reduces the risk of overfitting through two key mechanisms: bagging and feature randomness. Bagging involves training multiple decision trees on different subsets of the training data, which helps in reducing the variance of the model and making it more stable. Additionally, during the construction of each decision tree, only a random subset of features is considered for splitting at each node. This feature randomness further contributes to reducing overfitting by decorrelating the trees and preventing them from relying too heavily on a specific set of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each decision tree in a Random Forest Regressor independently predicts the target value based on the input features. The final prediction of the Random Forest Regressor is obtained by aggregating (averaging) the predictions of all individual trees. This ensemble approach helps to mitigate the errors and biases of individual trees, leading to a more accurate overall prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What are the hyperparameters of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common hyperparameters of the Random Forest Regressor include:\n",
    "\n",
    "Number of trees (n_estimators)\n",
    "Maximum depth of trees (max_depth)\n",
    "Minimum number of samples required to split an internal node (min_samples_split)\n",
    "Minimum number of samples required to be at a leaf node (min_samples_leaf)\n",
    "Maximum number of features to consider for the best split (max_features)\n",
    "Criterion for measuring the quality of a split (criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor and Decision Tree Regressor are both algorithms used for regression tasks, but they differ in how they make predictions:\n",
    "\n",
    "Decision Tree Regressor consists of a single tree that makes predictions based on a sequence of feature-based decisions.\n",
    "Random Forest Regressor is an ensemble of multiple Decision Tree Regressors. It reduces overfitting and increases predictive accuracy by aggregating the predictions of multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. What are the advantages and disadvantages of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "Better generalization due to ensemble learning.\n",
    "Reduced overfitting through bagging and feature randomness.\n",
    "Handles a mix of numeric and categorical features well.\n",
    "Robust to outliers and noisy data.\n",
    "Disadvantages:\n",
    "\n",
    "Complexity and computational cost increase with the number of trees.\n",
    "Harder to interpret compared to a single decision tree.\n",
    "Might not perform well on very small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What is the output of Random Forest Regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a Random Forest Regressor is a continuous numeric value, which is the aggregated prediction from all the individual decision trees in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Random Forest can be used for classification tasks as well. In that case, it's called a Random Forest Classifier. The underlying principle is the same: an ensemble of decision trees is used to make predictions, but for classification, the majority class among the trees' predictions is taken as the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
