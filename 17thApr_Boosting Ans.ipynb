{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is an ensemble learning method that combines the predictions from multiple weak learners (typically decision trees) to create a strong predictive model. Gradient Boosting Regression aims to minimize the error between the predicted and actual values by iteratively fitting new models to the residuals (the differences between predictions and actual values) of the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. \n",
    "Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Generate a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Initialize the prediction with the mean of target values\n",
    "pred = np.mean(y)\n",
    "\n",
    "# Number of trees in the ensemble\n",
    "n_trees = 100\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(n_trees):\n",
    "    # Calculate residuals\n",
    "    residuals = y - pred\n",
    "    \n",
    "    # Fit a decision tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=2)\n",
    "    tree.fit(X.reshape(-1, 1), residuals)\n",
    "    \n",
    "    # Make predictions with the current tree and update the ensemble prediction\n",
    "    tree_pred = tree.predict(X.reshape(-1, 1))\n",
    "    pred += learning_rate * tree_pred\n",
    "\n",
    "# Your ensemble model is ready, and 'pred' contains the final prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 50}\n",
      "Mean Squared Error: 0.5172019250760084\n",
      "R-squared: 0.5689983957699931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Create a Gradient Boosting Regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best trained model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a simple model, often a decision tree with limited depth (shallow tree), that performs slightly better than random guessing on a given task. Weak learners are the building blocks used in the ensemble. In each boosting iteration, a weak learner is trained to correct the errors made by the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictive performance of an ensemble of weak learners. It does this by focusing on the mistakes made by the previous models. In each iteration, a new weak learner is added to the ensemble to correct the errors made by the existing ensemble. This process continues until a predefined number of iterations or until a performance criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, a simple model (usually a shallow decision tree) is fit to the data.\n",
    "The residuals (the differences between the predictions and actual values) of this model are calculated.\n",
    "A new weak learner is fit to these residuals to predict the remaining errors.\n",
    "The predictions of all weak learners in the ensemble are combined to make the final prediction.\n",
    "This process is repeated iteratively, with each new weak learner focusing on the errors of the previous ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with an initial prediction, often the mean of the target values.\n",
    "Calculate the residuals, which are the differences between the current predictions and the actual target values.\n",
    "Fit a weak learner (e.g., a decision tree) to the residuals to capture the patterns in the errors.\n",
    "Adjust the prediction by adding a fraction of the predictions made by the weak learner to the current predictions. This fraction is controlled by the learning rate.\n",
    "Repeat steps 2-4 for a specified number of iterations, gradually reducing the errors in the predictions.\n",
    "The final prediction is the sum of the initial prediction and the contributions from all weak learners.\n",
    "By minimizing the residuals in each step, Gradient Boosting builds an ensemble of weak learners that work together to improve predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
