{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a technique used in data preprocessing to transform the features of a dataset to a specific range, typically between 0 and 1. It's done by subtracting the minimum value of the feature and then dividing by the range (difference between maximum and minimum values).\n",
    "\n",
    "Example:\n",
    "Let's say you have a dataset of house prices, and the price values range from $100,000 to $1,000,000. By applying Min-Max scaling, you can transform these values to a range between 0 and 1. If a house has a price of $400,000, after Min-Max scaling, it would become (400,000 - 100,000) / (1,000,000 - 100,000) = 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, scales the feature vector of each data point to have a unit norm (length). It divides each feature value by the Euclidean norm (magnitude) of the feature vector.\n",
    "\n",
    "Example:\n",
    "Consider a dataset of student exam scores with two features: Math and English scores. Let's say one student has Math score 80 and English score 60. The Euclidean norm of their feature vector is √(80^2 + 60^2) ≈ 100. So, after unit vector scaling, the normalized feature vector would be (80/100, 60/100) = (0.8, 0.6).\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is that the former focuses on the direction of the data points while the latter focuses on the range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional space while retaining as much variance as possible. It achieves this by identifying the principal components, which are orthogonal linear combinations of the original features.\n",
    "\n",
    "Example:\n",
    "Imagine a dataset of 2D points scattered across a plane. PCA would find the directions (principal components) along which the data varies the most. The first principal component is the direction of maximum variance, and the second principal component is orthogonal to the first and represents the second most important direction of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a type of feature extraction technique. Feature extraction involves transforming the original features into a new set of features while preserving relevant information. PCA achieves this by creating new features that are linear combinations of the original features, where the new features capture the most important variations in the data.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with various features related to a person's physical attributes like height, weight, arm length, leg length, etc. You can apply PCA to reduce these features to a smaller set of principal components that represent the most significant variations in the data. These components could be combinations of height, weight, arm length, etc., that capture the most important patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a recommendation system, Min-Max scaling can be used to standardize the features so that they have similar scales, preventing any one feature from dominating the recommendation process due to its larger range. Here's how you would use Min-Max scaling for the given features:\n",
    "\n",
    "Identify the features to be scaled: In this case, the features are price, rating, and delivery time.\n",
    "\n",
    "Calculate the minimum and maximum values for each feature across the dataset.\n",
    "\n",
    "Apply Min-Max scaling formula to each feature:\n",
    "\n",
    "Scaled Value = (Original Value - Min Value) / (Max Value - Min Value)\n",
    "The scaled values will now range between 0 and 1, making them suitable for the recommendation process. This ensures that no single feature disproportionately influences the recommendations due to its scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a project involving predicting stock prices, PCA can be used to reduce the dimensionality of the dataset, making it easier to work with and potentially improving the model's performance. Here's how you could use PCA:\n",
    "\n",
    "Gather the features: Collect all the relevant features from the dataset, such as company financial metrics (e.g., revenue, profit, debt), market trends (e.g., interest rates, inflation), and any other relevant data.\n",
    "\n",
    "Standardize the features: Before applying PCA, it's important to standardize the features to have zero mean and unit variance. This ensures that no single feature dominates the PCA process due to its scale.\n",
    "\n",
    "Apply PCA: Calculate the principal components of the standardized feature matrix. These components are linear combinations of the original features that capture the most important variations in the data.\n",
    "\n",
    "Determine the number of principal components: You can decide how many principal components to retain based on the explained variance. Retaining a sufficient number of components that explain a high percentage of the total variance is important to retain meaningful information.\n",
    "\n",
    "Project the data: Transform the original dataset into the reduced-dimensional space spanned by the retained principal components.\n",
    "\n",
    "Train your model: Use the reduced-dimensional dataset as input to train your stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Value: 1\n",
    "Max Value: 20\n",
    "\n",
    "Scaled Value = (Original Value - Min Value) / (Max Value - Min Value)\n",
    "\n",
    "For each value:\n",
    "\n",
    "Scaled Value = (1 - 1) / (20 - 1) = 0\n",
    "Scaled Value = (5 - 1) / (20 - 1) = 0.25\n",
    "Scaled Value = (10 - 1) / (20 - 1) = 0.5\n",
    "Scaled Value = (15 - 1) / (20 - 1) = 0.75\n",
    "Scaled Value = (20 - 1) / (20 - 1) = 1\n",
    "So, the scaled values in the range of -1 to 1 would be: [0, 0.25, 0.5, 0.75, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of principal components to retain depends on the level of variance you want to preserve in the data. Typically, you would choose the number of components that collectively explain a high percentage of the total variance. A common approach is to set a threshold, such as retaining components that explain, for example, 95% or 99% of the variance.\n",
    "\n",
    "To determine the number of principal components to retain, you would follow these steps:\n",
    "\n",
    "Standardize the features: Ensure that all features have zero mean and unit variance.\n",
    "\n",
    "Calculate PCA: Perform PCA on the standardized data to obtain the explained variance ratios associated with each principal component.\n",
    "\n",
    "Analyze explained variance: Plot the cumulative explained variance as a function of the number of principal components. This will help you understand how many components are needed to capture a desired level of variance.\n",
    "\n",
    "Set a threshold: Based on the plot, choose the number of components that retain a sufficiently high percentage of the total variance while reducing the dimensionality.\n",
    "\n",
    "For example, if the cumulative explained variance plot shows that the first two components explain around 90% of the variance, you might choose to retain those two components. However, the specific number of components to retain depends on the characteristics of your data and the trade-off between dimensionality reduction and information preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
