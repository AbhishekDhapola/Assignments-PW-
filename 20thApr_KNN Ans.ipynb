{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and widely used supervised machine learning algorithm that can be used for both classification and regression tasks. KNN works based on the principle that data points with similar attributes tend to belong to the same class or have similar output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the value for 'K' in KNN is crucial. A smaller 'K' value makes the algorithm more sensitive to noise in the data, potentially leading to overfitting. On the other hand, a larger 'K' value can make the algorithm less sensitive to local variations but might also result in underfitting. The optimal 'K' value depends on the dataset and problem at hand. It's often determined using techniques like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN Classifier is used for classification tasks where the goal is to predict the class membership of an input data point. KNN Regressor, on the other hand, is used for regression tasks where the goal is to predict a continuous numeric value. KNN Classifier assigns a class label to the input based on the majority class of its 'K' nearest neighbors, while KNN Regressor predicts the output value as the average (or weighted average) of the 'K' nearest neighbors' output values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of KNN is typically measured using appropriate metrics depending on the task. For classification, metrics like accuracy, precision, recall, F1-score, and ROC curves are commonly used. For regression, metrics like mean squared error (MSE), root mean squared error (RMSE), and R-squared are commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the phenomenon where the effectiveness of certain algorithms, like KNN, deteriorates as the number of dimensions (features) in the dataset increases. In high-dimensional spaces, data points become sparse, and the concept of distance loses its meaning, making it difficult for KNN to find meaningful neighbors. This can lead to poor performance and increased computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing values in KNN involves imputing the missing values before applying the algorithm. One common approach is to replace missing values with the mean or median of the available values in the feature. Another approach is to assign a special value to missing data points and treat them accordingly during distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both KNN classifier and regressor have their strengths and weaknesses. KNN classifier is suitable for problems where the output is categorical and class-based, like image classification, text categorization, etc. KNN regressor is appropriate for problems where the output is continuous and numeric, like predicting housing prices, stock prices, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths:\n",
    "\n",
    "Simple and easy to understand.\n",
    "No assumptions about the underlying data distribution.\n",
    "Can handle multi-class problems.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive for large datasets.\n",
    "Sensitive to the choice of 'K'.\n",
    "Prone to noise and outliers.\n",
    "To address these weaknesses, techniques like dimensionality reduction, distance weighting, and optimized search structures (like KD-trees) can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distance measures the straight-line distance between two points in a Euclidean space. It's calculated as the square root of the sum of squared differences between corresponding coordinates.\n",
    "\n",
    "Manhattan distance (also known as L1 distance or taxicab distance) measures the distance between two points by summing up the absolute differences between their corresponding coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling is important in KNN because the algorithm relies on distance metrics to determine the \"closeness\" of data points. If features are on different scales, those with larger ranges can dominate the distance calculation. Scaling ensures that all features contribute equally to distance calculations, preventing bias toward specific features. Common scaling techniques include z-score normalization or Min-Max scaling.\n",
    "\n",
    "Remember that while KNN is a versatile algorithm, its performance depends heavily on the data and problem context. It's a good practice to try different algorithms and evaluate their performance to choose the best approach for your specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
