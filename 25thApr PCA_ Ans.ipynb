{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are concepts from linear algebra that are associated with square matrices.\n",
    "An eigenvalue (λ) of a square matrix A is a scalar such that when A is multiplied by a corresponding eigenvector (v), the result is a scaled version of the eigenvector: Av = λv.\n",
    "In other words, an eigenvector remains in the same direction (up to scaling) when the matrix is applied to it.\n",
    "The eigen-decomposition approach involves decomposing a matrix A into a set of its eigenvalues and eigenvectors. It is represented as A = PDP^(-1), where P is the matrix of eigenvectors, D is the diagonal matrix of eigenvalues, and P^(-1) is the inverse of P.\n",
    "Example:\n",
    "Let's say we have a 2x2 matrix A:\n",
    "A = [[3, 1],\n",
    "[0, 2]]\n",
    "To find the eigenvalues, we solve the characteristic equation: det(A - λI) = 0, where I is the identity matrix.\n",
    "For A, the characteristic equation becomes:\n",
    "|3-λ 1 | |λ 0| |(3-λ)λ - 10| = 0\n",
    "|0 2-λ| * |0 λ| = |0(2-λ) - (λ*0)| = 0\n",
    "\n",
    "Solving this equation yields two eigenvalues: λ1 = 3 and λ2 = 2.\n",
    "\n",
    "To find the corresponding eigenvectors, plug each eigenvalue back into the equation Av = λv and solve for v:\n",
    "For λ1 = 3:\n",
    "(A - 3I)v1 = 0\n",
    "|0 1 | |x| |0|\n",
    "|0 -1 | * |y| = |0|\n",
    "The solution is v1 = [1, 0].\n",
    "\n",
    "For λ2 = 2:\n",
    "(A - 2I)v2 = 0\n",
    "|1 1 | |x| |0|\n",
    "|0 -2 | * |y| = |0|\n",
    "The solution is v2 = [-1, 0].\n",
    "\n",
    "So, the eigenvalues of A are λ1 = 3 and λ2 = 2, and the corresponding eigenvectors are v1 = [1, 0] and v2 = [-1, 0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen decomposition is a process of breaking down a square matrix A into a set of its eigenvalues and eigenvectors.\n",
    "Mathematically, A can be represented as A = PDP^(-1), where P is the matrix of eigenvectors, D is the diagonal matrix of eigenvalues, and P^(-1) is the inverse of P.\n",
    "It is significant in linear algebra because it simplifies many matrix operations, making it easier to analyze and manipulate matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A square matrix A is diagonalizable using the eigen-decomposition approach if and only if it has a complete set of linearly independent eigenvectors.\n",
    "Proof: A matrix A is diagonalizable if it can be expressed as A = PDP^(-1), where P is the matrix of eigenvectors. P can only be inverted if its columns (the eigenvectors) are linearly independent. If they are not independent, P^(-1) does not exist, and A is not diagonalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spectral theorem states that for any symmetric matrix (a special type of square matrix), the eigenvectors are orthogonal (perpendicular) to each other, and the eigenvalues are real.\n",
    "This theorem is significant in the context of eigen-decomposition because it guarantees that for symmetric matrices, the diagonalization results in real eigenvalues and orthogonal eigenvectors, simplifying various mathematical operations.\n",
    "Example: Consider a symmetric matrix A:\n",
    "A = [[4, 1],\n",
    "[1, 3]]\n",
    "Its eigenvalues are λ1 = 5 and λ2 = 2, and the corresponding normalized eigenvectors are v1 = [0.92, 0.38] and v2 = [-0.38, 0.92]. These eigenvectors are orthogonal (dot product = 0) and correspond to real eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find eigenvalues, solve the characteristic equation det(A - λI) = 0, where A is the matrix, λ is the eigenvalue, and I is the identity matrix.\n",
    "The solutions to this equation are the eigenvalues of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are the vectors that, when multiplied by a matrix, result in a scaled version of themselves.\n",
    "In the equation Av = λv, v is the eigenvector, A is the matrix, λ is the eigenvalue, and the product Av is a scaled version of v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors represent directions in space that are preserved (scaled) by the linear transformation defined by the matrix A.\n",
    "Eigenvalues represent the scaling factor by which the eigenvectors are stretched or shrunk.\n",
    "In the case of 2D or 3D space, eigenvectors can be thought of as the principal axes along which data spreads, and eigenvalues represent the variance along those axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) in data analysis and dimensionality reduction.\n",
    "Vibrational analysis in structural engineering and physics.\n",
    "Quantum mechanics, where eigenstates represent the possible states of a quantum system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, a matrix can have multiple sets of eigenvectors and eigenvalues.\n",
    "Different sets of eigenvectors and eigenvalues may be associated with different representations or bases for the same matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA): PCA uses eigen-decomposition to reduce the dimensionality of data while preserving as much variance as possible. It finds the principal components (eigenvectors) and their importance (eigenvalues) in the data, allowing for effective data compression and visualization.\n",
    "\n",
    "Spectral Clustering: In spectral clustering, eigen-decomposition is used to transform the data into a new space where clustering is easier. The eigenvalues and eigenvectors of a similarity matrix are employed to identify clusters in the data.\n",
    "\n",
    "Recommender Systems: Eigen-decomposition techniques like Singular Value Decomposition (SVD) are used in collaborative filtering-based recommender systems. SVD decomposes the user-item interaction matrix into matrices of user and item latent factors, which can be used to make personalized recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
