{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression involves predicting a dependent variable (target) based on a single independent variable (feature). The relationship between the two variables is modeled as a straight line. It can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = β₀ + β₁x + ε\n",
    "Where:\n",
    "\n",
    "y: Dependent variable\n",
    "x: Independent variable\n",
    "β₀: Intercept\n",
    "β₁: Slope\n",
    "ε: Error term\n",
    "Example of Simple Linear Regression: Predicting a person's weight (y) based on their height (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression, on the other hand, involves predicting the dependent variable based on two or more independent variables. The relationship is modeled as a linear combination of these variables. The equation is extended to accommodate multiple predictors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
    "Where:\n",
    "\n",
    "x₁, x₂, ..., xₙ: Independent variables\n",
    "β₁, β₂, ..., βₙ: Coefficients for each independent variable\n",
    "Example of Multiple Linear Regression: Predicting a house's price (y) based on its size (x₁) and number of bedrooms (x₂)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity: The relationship between independent and dependent variables is linear. You can use scatter plots and residual plots to assess this assumption.\n",
    "\n",
    "Independence: Residuals should be independent of each other. You can use residual plots and Durbin-Watson test to check this.\n",
    "\n",
    "Homoscedasticity: The variance of residuals should be constant across all levels of independent variables. Residual plots can help identify heteroscedasticity.\n",
    "\n",
    "Normality: Residuals should follow a normal distribution. This can be checked using Q-Q plots and normality tests.\n",
    "\n",
    "No Multicollinearity: Independent variables should not be highly correlated. Variance Inflation Factor (VIF) can help detect multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept (β₀): It's the predicted value of the dependent variable when all independent variables are zero.\n",
    "Slope (β₁): It represents the change in the dependent variable for a unit change in the independent variable, holding other variables constant.\n",
    "Example: In a simple linear regression where weight (y) is regressed on height (x), the intercept might represent the weight of a person with zero height (not practically meaningful), and the slope might indicate the average weight increase for each unit increase in height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is an optimization algorithm used in machine learning to minimize a loss function. It iteratively adjusts model parameters to find the optimal values that minimize the error between predicted and actual values. It works by calculating the gradient of the loss function with respect to the parameters and updating the parameters in the opposite direction of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is an extension of simple linear regression that involves predicting a dependent variable using multiple independent variables. The model equation becomes a linear combination of these variables. The goal is to find the coefficients that best fit the data while minimizing the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when independent variables in a multiple regression model are highly correlated, making it difficult to isolate their individual effects. It can lead to unstable coefficient estimates and difficulties in interpreting the model. VIF is used to detect multicollinearity – higher VIF values indicate higher multicollinearity.\n",
    "\n",
    "To address multicollinearity, you can:\n",
    "\n",
    "Remove one of the correlated variables.\n",
    "Combine the correlated variables into a single variable.\n",
    "Use dimensionality reduction techniques like Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression extends linear regression by allowing the relationship between variables to be modeled as an nth-degree polynomial. The equation becomes a polynomial function of the independent variable(s), which can fit more complex curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "Can model complex relationships that linear regression can't capture.\n",
    "Flexible in fitting data with curvatures.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to overfitting, especially with high-degree polynomials.\n",
    "Can produce erratic results outside the data range.\n",
    "Interpretability becomes challenging as the model complexity increases.\n",
    "Use Polynomial Regression when:\n",
    "\n",
    "The relationship between variables seems to be nonlinear.\n",
    "You have domain knowledge suggesting a polynomial relationship.\n",
    "You have a small dataset where a simple linear model underperforms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
