{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator,\" is a type of linear regression that incorporates regularization. It adds a penalty term to the linear regression's cost function, which is a combination of the sum of squared differences between the predicted and actual values (the residual sum of squares) and the absolute values of the regression coefficients multiplied by a tuning parameter (lambda).\n",
    "\n",
    "The main difference between Lasso Regression and other regression techniques, such as ordinary least squares (OLS) regression, is the introduction of this regularization term. This term encourages the model to not only fit the data well but also to keep the magnitude of the coefficients small, effectively pushing some coefficients to exactly zero. This makes Lasso Regression useful for feature selection and producing more interpretable models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of Lasso Regression in feature selection is its ability to automatically perform feature selection by driving some coefficients to zero. This means that Lasso can effectively identify and exclude irrelevant or redundant features from the model, leading to a simpler and more interpretable model. This feature selection property is particularly useful when dealing with high-dimensional data with many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting coefficients in regular linear regression. The coefficients represent the change in the target variable associated with a one-unit change in the corresponding predictor, while holding other predictors constant. However, due to the regularization, some coefficients might be exactly zero, indicating that the corresponding features are not contributing to the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main tuning parameter in Lasso Regression is the regularization parameter, often denoted as lambda (Î»). This parameter controls the strength of the regularization penalty. A smaller lambda allows the model to fit the data more closely, while a larger lambda increases the regularization effect, leading to more coefficients being pushed towards zero. The choice of lambda affects the trade-off between fitting the data well and keeping the model simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression is fundamentally a linear regression technique. However, it can be used in conjunction with non-linear transformations of the features to address non-linearity in the data. For example, you can create polynomial features by raising the original features to different powers, and then apply Lasso Regression to the extended feature set. This allows the model to capture non-linear relationships, but the final model is still linear in terms of the transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques, but they differ in the type of regularization they apply. Ridge Regression adds a penalty term based on the sum of squared coefficients (L2 norm), while Lasso Regression adds a penalty term based on the sum of the absolute values of coefficients (L1 norm). Consequently, Ridge Regression tends to shrink all coefficients towards zero, but they rarely become exactly zero, while Lasso Regression can lead to exact zero coefficients, effectively performing feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can help with multicollinearity to some extent due to its feature selection property. When there's multicollinearity (high correlation) among input features, Lasso Regression may select one feature over another, effectively ignoring the less relevant one and reducing the impact of multicollinearity on the model. However, it's worth noting that Lasso Regression might not always fully eliminate multicollinearity-related issues, and Ridge Regression is often preferred when multicollinearity is a significant concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the optimal lambda in Lasso Regression typically involves techniques like cross-validation. You would split your dataset into training and validation sets, fit the Lasso Regression model with different values of lambda on the training set, and then evaluate the model's performance on the validation set using a suitable metric (e.g., mean squared error). The lambda that gives the best performance on the validation set is selected as the optimal value. Grid search or more advanced optimization techniques can also be used to automate this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
